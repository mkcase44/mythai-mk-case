{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8ahCkHvQrOr"
      },
      "source": [
        "<h1><center></center></h1>\n",
        "<div style=\"display: flex; justify-content: center; margin: 0 auto;\" align=\"center\">\n",
        "  <img src=\"https://myth-ai.com/wp-content/uploads/2023/05/646f153be1e56.png\" href=\"https://myth-ai.com/\" width=\"100px\" align=\"center\">\n",
        "  <h1>Technical Assignment</h1>\n",
        "</div>\n",
        "\n",
        "<div align=\"center\">\n",
        "  <h2>\n",
        "  Sketch Generation via Diffusion Models using Sequential Strokes\n",
        "  </h2>\n",
        "</div>\n",
        "\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://github.com/googlecreativelab/quickdraw-dataset/blob/master/preview.jpg?raw=true\">\n",
        "  <figcaption>\n",
        "    Collection of 50 million drawings across 345 categories, contributed by players of the game Quick, Draw!. Drawings were captured as timestamped vectors.\n",
        "    <i>Source: <a href=\"https://quickdraw.withgoogle.com/data/\">Quick, Draw! Dataset</a>.</i>\n",
        "  </figcaption>\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "## Objective\n",
        "\n",
        "In this project, you are expected to implement a **conditional generative diffusion model** that learns to generate hand-drawn sketches in a **stroke-by-stroke** sequential manner. Rather than generating the entire sketch at once, your model should mimic the **sequential nature of human drawing**, producing strokes one after another in a realistic and interpretable way.\n",
        "\n",
        "You will use the [Quick, Draw!](https://quickdraw.withgoogle.com/data/) dataset released by Google, which provides timestamped vector representations of user-drawn sketches across 345 object categories.\n",
        "\n",
        "---\n",
        "\n",
        "## Brief Explanation\n",
        "\n",
        "You will design and train a **separate conditional diffusion model** for each of the following three categories:\n",
        "\n",
        "- `cat`\n",
        "- `bus`\n",
        "- `rabbit`\n",
        "\n",
        "Each model must learn to generate sketches from that category using **sequential stroke data**. That means you will build **three separate models** in total‚Äîone per category.\n",
        "\n",
        "Your implementation must be documented in a reproducible Jupyter notebook, including training steps, visualizations, and both qualitative and quantitative evaluations.\n",
        "\n",
        "- Include comprehensive documentation of your approach and design decisions.\n",
        "- Provide clear training procedures, model architecture explanations, and inference code.\n",
        "- Ensure full reproducibility (running all cells should yield consistent results with fixed random seeds).\n",
        "\n",
        "---\n",
        "\n",
        "## Data Specification\n",
        "\n",
        "The Quick, Draw! dataset includes over 50 million sketches in vector format, with each sketch consisting of multiple strokes, where each stroke is a sequence of coordinates (`x`, `y`) along with timing information.\n",
        "\n",
        "You can download the raw `.ndjson` files from the this [section](#cell-id1). The following commands will download the required categories (`cat`, `bus`, `rabbit`) into the ./data directory.\n",
        "\n",
        "**‚ö†Ô∏è Note:** If you're not using Google Colab or Kaggle, make sure you have `gsutil` installed. You can install it via pip:\n",
        "\n",
        "```bash\n",
        "pip install gsutil\n",
        "```\n",
        "\n",
        "**‚ö†Ô∏è Important:** The dataset files are in [NDJSON](https://github.com/ndjson/ndjson-spec) format. Make sure to install the ndjson Python module before attempting to parse the files.\n",
        "\n",
        "```bash\n",
        "pip install ndjson\n",
        "```\n",
        "\n",
        "### Train/Test Subsets for Target Categories\n",
        "\n",
        "After downloading the dataset in the `./data` directory, extract the provided `subset.zip` file. This archive includes the predefined train/test splits for each of the three categories.\n",
        "\n",
        "```\n",
        "subset/\n",
        "‚îú‚îÄ‚îÄ cat/\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ indices.json\n",
        "‚îú‚îÄ‚îÄ bus/\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ indices.json\n",
        "‚îî‚îÄ‚îÄ rabbit/\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ indices.json\n",
        "```\n",
        "\n",
        "Each `indices` file contains a JSON structure with two keys:\n",
        "\n",
        "- `\"train\"`: list of indices for training\n",
        "- `\"test\"`: list of indices for testing\n",
        "\n",
        "**‚ö†Ô∏è Important:** Strictly adhere to these predefined splits for consistent evaluation.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Evaluation\n",
        "\n",
        "You must evaluate your model both **qualitatively** and **quantitatively**.\n",
        "\n",
        "### Quantitative Evaluation\n",
        "\n",
        "Use the following metrics to compare the real test set sketches with those generated by your model:\n",
        "\n",
        "- **FID (Fr√©chet Inception Distance)**\n",
        "- **KID (Kernel Inception Distance)**\n",
        "\n",
        "These metrics should be computed **separately for each category** using the sketches indexed under the `\"test\"` key in each category‚Äôs `indices.json` file.\n",
        "\n",
        "> **Final submission must include three FID and three KID scores‚Äîone pair per category.**\n",
        "\n",
        "### Qualitative Evaluation\n",
        "\n",
        "Provide visual demonstrations including:\n",
        "\n",
        "- Sample generated sketches for each category.\n",
        "- Your submission must include three animated GIFs (one per category) showing the stroke-by-stroke generation process, similar to `example.gif` file in the link.\n",
        "- Comparison between real and generated sketches.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Deliverables\n",
        "\n",
        "Your submission should include the following:\n",
        "\n",
        "- A well-structured **Jupyter Notebook** that:\n",
        "  - Explains your approach and design decisions\n",
        "  - Implements the conditional diffusion model\n",
        "  - Includes training procedure and inference pipeline code\n",
        "  - Presents both qualitative and quantitative results\n",
        "  - Visual examples of generated sketches for each of the 3 categories\n",
        "  - Animated GIFs demonstrating progressive sketch generation (similar to the provided example.gif)\n",
        "  - Clearly computed FID/KID scores for each category\n",
        "- Model performance analysis across categories\n",
        "- Comparison of generated vs. real sketch characteristics\n",
        "- Discussion of limitations and potential improvements\n",
        "\n",
        "\n",
        "> üîí All visualizations must be based on sketches generated by your own model. Using samples from external sources will be considered **plagiarism** and will result in disqualification.\n",
        "\n",
        "> üîÅ The notebook must be **fully reproducible**: running all cells from top to bottom should produce the same results (assuming fixed random seed).\n",
        "\n",
        "---\n",
        "\n",
        "## Acknowledgements\n",
        "\n",
        "- [The Quick, Draw! Dataset](https://github.com/googlecreativelab/quickdraw-dataset)\n",
        "- [Quick, Draw! Kaggle Competition](https://www.kaggle.com/c/quickdraw-doodle-recognition/overview)\n",
        "- [Diffusion Models Overview (Lil‚ÄôLog)](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)\n",
        "- [Ha, D., & Eck, D. (2017). A neural representation of sketch drawings. arXiv preprint arXiv:1704.03477.](https://arxiv.org/pdf/1704.03477)\n",
        "- Special thanks to M. Sung, KAIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lfaHsLNkuYB"
      },
      "source": [
        "# Download the Quick, Draw! Dataset\n",
        "\n",
        "<a name=\"cell-id1\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt2OjtIbWOAQ"
      },
      "outputs": [],
      "source": [
        "# If you're not using Colab or Kaggle, uncomment the following line:\n",
        "# !pip install gsutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue4Lavg4XzrP"
      },
      "outputs": [],
      "source": [
        "%pip install ndjson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRD7IDkp3ltZ",
        "outputId": "b4c53999-19e1-4f54-dd60-93f2c6d6fd81"
      },
      "outputs": [],
      "source": [
        "%mkdir data\n",
        "!gsutil -m cp 'gs://quickdraw_dataset/full/simplified/cat.ndjson' ../data\n",
        "!gsutil -m cp 'gs://quickdraw_dataset/full/simplified/bus.ndjson' ../data\n",
        "!gsutil -m cp 'gs://quickdraw_dataset/full/simplified/rabbit.ndjson' ../data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M00-fIER2U1-"
      },
      "source": [
        "# Solution\n",
        "\n",
        "- Briefly explain why you chose the method you did.\n",
        "- Discuss the drawbacks and advantages of your chosen method.\n",
        "- Evaluate and discuss the results for each metric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook involved the technical development of a diffusion models. There are different report which contains less technical detail and related works (general_report.pdf).\n",
        "\n",
        "In this assignment, diffusion models designed to produce hand-drawn sketches in a sequential, stroke-by-stroke manner. To facilitate this process, the raw Quick, Draw! data was converted into a 5D vector representation (Œîx, Œîy, pen_state). There are couple of architecture with different conditions were experimeneted includes Diffusion Transformer and LSTM based Stroke History encoder.\n",
        "\n",
        "<b>While the end-to-end training pipeline was successfully built, the model faced significant challenges. <u>Although the final visual outputs have not yet converged, this notebook provides a comprehensive record of the architectural design, the  training and evaluation process.</u></b>\n",
        "\n",
        "\n",
        "In next sections Dataset Processing, Diffusion Models, Training Strategy and Evaluation methods will be discussed detailly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Processing\n",
        "\n",
        "This project focuses on the Quick, Draw! dataset, which contains sketches from various categories. For this assignment, I will work with only three specific classes: cat, bus, and rabbit. Before delving into the diffusion model architecture and training strategy, I will use several helper functions to gain a deeper understanding of the data. The insights gathered here will be essential for the subsequent sections of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CONSTANTS\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "CLASS_NAME = \"cat\" # cat, bus, rabbit are available\n",
        "NDJSON_PATH = f\"../data/{CLASS_NAME}.ndjson\"\n",
        "SET_INDICES_PATH = f\"../subset/{CLASS_NAME}/indices.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training and test sets are divided and shared, therefore these splits are directly used for model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the data files\n",
        "from src.utils.dataset_utils import read_ndjson_file, get_subset\n",
        "\n",
        "dataset = read_ndjson_file(file_path=NDJSON_PATH)\n",
        "train_set = get_subset(dataset=dataset, indices_json_path=SET_INDICES_PATH, subset_name=\"train\")\n",
        "test_set = get_subset(dataset=dataset, indices_json_path=SET_INDICES_PATH, subset_name=\"test\")\n",
        "\n",
        "print(f\"Total Training Set Size: {len(train_set)}\")\n",
        "print(f\"Total Testing Set Size: {len(test_set)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "from IPython.display import Image\n",
        "from src.utils.dataset_utils import generate_gif\n",
        "\n",
        "# Randomly select a drawing from the training set and generate a GIF\n",
        "randomly_selected_drawing = random.choice(train_set)\n",
        "randomly_selected_drawing_idx = train_set.index(randomly_selected_drawing)\n",
        "\n",
        "saved_path = generate_gif(drawing=randomly_selected_drawing,\n",
        "                          output_path=\"./\",\n",
        "                          output_name=\"random_drawing.gif\")\n",
        "\n",
        "Image(filename=saved_path, width=300, height=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Quick, Draw! dataset provides sketches as a sequence of strokes, with each stroke being an array of (x, y) coordinates. This structure makes it possible to render the final sketch sequentially, just as a human would draw it.\n",
        "\n",
        "However, feeding this raw coordinate data directly into a deep learning network is not ideal, as its absolute structure makes the learning process more difficult. [SketchRNN](https://arxiv.org/abs/1704.03477) paper introduced a method to convert this data into a more effective 5D vector format. In this format, each point is represented by five elements: the first two (Œîx, Œîy) are the offset from the previous point, while the last three are a one-hot vector representing the pen's state. The third element is 1 if the pen is down (stroke is being drawn), the fourth is 1 if the pen is up (end of a stroke), and the final element is 1 to signify the end of the entire drawing.\n",
        "\n",
        "After this conversion, the dataset is properly structured to be fed into a deep learning network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert dataset to 5D format\n",
        "from src.utils.dataset_utils import convert_drawing_to_5d_format\n",
        "\n",
        "# (delta_x, delta_y, pen_down, pen_up, completed)\n",
        "train_set_5d = convert_drawing_to_5d_format(train_set)\n",
        "test_set_5d = convert_drawing_to_5d_format(test_set)\n",
        "\n",
        "randomly_selected_drawing = random.choice(train_set_5d)\n",
        "print(f\"5D Format Shape: {randomly_selected_drawing.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before diving into the training process, it's a good practice to verify that the data conversion works seamlessly. To do this, I have implemented a function that converts the 5D vector format back to the original coordinate format. This \"round-trip\" conversion test ensures that our preprocessing pipeline is reversible and correct before we proceed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 5D converting format\n",
        "from src.utils.dataset_utils import convert_5d_to_raw_format\n",
        "\n",
        "raw_randomly_selected_drawing = convert_5d_to_raw_format(train_set_5d[randomly_selected_drawing_idx])\n",
        "saved_path = generate_gif(drawing=raw_randomly_selected_drawing,\n",
        "                          output_path=\"./\",\n",
        "                          output_name=\"regenerated_drawing.gif\")\n",
        "\n",
        "Image(filename=saved_path, width=300, height=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, normalizing the data is a crucial preprocessing step that helps stabilize and accelerate the training of deep learning models. For this project, I will adopt the strategy from the [SketchRNN](https://arxiv.org/abs/1704.03477) paper. This approach involves calculating the standard deviation of all delta values (Œîx, Œîy) across the entire training set and then normalizing the data by dividing each value by this single statistic.\n",
        "\n",
        "It is essential that the standard deviation is computed only from the training data. This same value must then be used to normalize the validation and test sets, and later to denormalize the model's output during inference. This practice prevents data leakage and ensures consistent scaling across all phases of the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the mean and std of the dataset for normalization\n",
        "from src.utils.dataset_utils import calculate_normalization_params\n",
        "\n",
        "mean, std  = calculate_normalization_params(train_set_5d)\n",
        "\n",
        "print(f\"Normalization Parameters for Training Dataset:\")\n",
        "print(f\"Mean: {mean}, Std: {std}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diffusion Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section I will deep dive the Dataset Object and Diffusion Models architecture. As it is mentioned before there is no succesfully trained network in the report. Therefore I decided to share each strategy that I used while training the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Constants\n",
        "import torch\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unconditional/Conditional DiT Approach\n",
        "In this section, I experimented with the same core architecture in different configurations. The main goal of this approach is to generate a complete sketch in a single sampling process.\n",
        "\n",
        "#### QuickDraw Dataset Object\n",
        "This custom Dataset object is initialized with the <i>drawings dataset</i>, <i>maximum sequence length</i>, <i>standart deviation of the training dataset</i>, <i>random scale factor for augmentation dataset</i>, <i>stroke augmentation probability</i> and <i>limit value to avoid outliers</i>. The dataset code can be review in <i>src/modules/dataset.py</i> file.\n",
        "\n",
        "The data processing pipeline is as follows:\n",
        "1. <b>Preprocessing</b>; First, drawings longer than the max_seq_length are filtered out. Then, outlier delta values are clipped, and finally, the data is normalized using the pre-calculated standard deviation.\n",
        "2. <b>__ getitem __</b>; When a sample is requested, it is randomly selected and augmented based on the provided parameters. A special SOS (Start of Sequence) token is prepended to the drawing, which is then padded* with zeros to the max_seq_length.\n",
        "\n",
        "<b>The SOS token, [0, 0, 1, 0, 0], signals the start of a drawing.</b> This helps the model learn that the first real delta values represent absolute coordinates, a method inspired by the [SketchRNN](https://arxiv.org/abs/1704.03477) paper. The network first sees zero deltas with a \"pen down\" state, which sets the condition for the following stroke data.\n",
        "\n",
        "I set 96 as a maximum sequence length. This value is copied from [SketchKnitter](https://openreview.net/pdf?id=4eJ43EN2g6l) paper which is designed for the same purpose.\n",
        "\n",
        "*Padding is essential for batching during model training, as it ensures all tensors in a batch have a uniform dimension. This allows the data to be processed in parallel on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create DataLoader for training and testing sets\n",
        "from torch.utils.data import DataLoader\n",
        "from src.modules.dataset import QuickDrawDataset\n",
        "\n",
        "\n",
        "train_dataset = QuickDrawDataset(\n",
        "    drawing_5d=train_set_5d,\n",
        "    max_seq_length=96, # Copies from the reference implementation\n",
        "    std=std,\n",
        "    random_scale_factor=0.15, # Copies from the reference implementation\n",
        "    augment_stroke_prob=0.10, # Copies from the reference implementation,\n",
        "    limit=1000, # Copies from the reference implementation\n",
        ")\n",
        "\n",
        "test_dataset = QuickDrawDataset(\n",
        "    drawing_5d=test_set_5d,\n",
        "    max_seq_length=96, # Copies from the reference implementation\n",
        "    std=std,\n",
        "    random_scale_factor=0.15, # Copies from the reference implementation\n",
        "    augment_stroke_prob=0.10, # Copies from the reference implementation,\n",
        "    limit=1000, # Copies from the reference implementation\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "for test_batch in train_dataloader:\n",
        "    print(f\"Batch Shape: {test_batch['stroke'].shape}\")\n",
        "    print(f\"Batch Sample: {test_batch['stroke'][0, :5]}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Diffusion Transformer Model\n",
        "I chose a Diffusion Transformer (DiT) for this task because Transformer networks are exceptionally well-suited for sequential problems. Their self-attention mechanism is powerful for capturing the long-range dependencies between points and strokes, which is crucial for generating coherent sketches.\n",
        "\n",
        "I experimented with two main variations of this architecture: an unconditional and a conditional approach. The model code can be review in <i>src/models/dit.py</i> file.\n",
        "\n",
        "##### Unconditional DiT\n",
        "In this approach, the model learns to generate a sketch holistically from a noised 2D vector. The architecture is designed with a multi-task learning objective:\n",
        "\n",
        "1. The delta values are noised and fed into the Transformer backbone.\n",
        "\n",
        "2. The model's final output layer is split into two separate \"heads\": one predicts the noise for the Œîx, Œîy values, and the other predicts the logits for the pen_state.\n",
        "\n",
        "This design, inspired by the multi-headed outputs in models like [SketchKnitter](https://openreview.net/pdf?id=4eJ43EN2g6l), allows the network to learn both the continuous motion and discrete state predictions from a shared underlying representation.\n",
        "\n",
        "![Unconditional Dit Architecture](assets/uncond_dit.png)\n",
        "\n",
        "##### Condional DiT\n",
        "\n",
        "This approach frames the task as an inpainting problem, where the pen states are treated as a known \"condition.\" The model's goal is to generate the correct movements (Œîx, Œîy) that correspond to this known sequence of actions.\n",
        "\n",
        "During training, only the Œîx, Œîy values are noised, while the pen_state vectors are kept clean.\n",
        "\n",
        "The model is fed both the noised deltas and the clean pen states as input.\n",
        "\n",
        "The model's objective is to predict the noise only for the delta values, using the clean pen states as a guiding context.\n",
        "\n",
        "This gives the model the explicit ability to decide how a stroke should look based on whether the pen is on the paper or being lifted.\n",
        "\n",
        "![Conditional Dit Architecture](assets/cond_dit.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Diffusion Model for First Approach\n",
        "from src.models.dit import DiffusionTransformer\n",
        "\n",
        "diffusion_model = DiffusionTransformer(\n",
        "    input_feats=5,\n",
        "    output_feats=2,\n",
        "    pen_state_feats=2,\n",
        "    pen_condition=True # Set to True for pen state conditioning\n",
        ")\n",
        "diffusion_model.to(DEVICE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stroke History based Diffusion Transformer Model\n",
        "In this section, I tried to use cross-attention layer by conditioning stroke history. This is an autoregressive diffusion approach. In each sampling step single stroke is generated and appended to the stroke history. Stroke History is an another LSTM based network which extracts features from the histroy and condition the DiT by cross attention layers.\n",
        "\n",
        "\n",
        "#### StrokeHistoryQuickDrawDataset Dataset Object\n",
        "This custom Dataset object is initialized with the <i>drawings dataset</i>, <i>maximum sequence length</i>, <i>standart deviation of the training dataset</i>, <i>random scale factor for augmentation dataset</i>, <i>stroke augmentation probability</i> and <i>limit value to avoid outliers</i>. The dataset code can be review in <i>src/modules/dataset.py</i> file.\n",
        "\n",
        "The data processing pipeline is as follows:\n",
        "1. <b>Preprocessing</b>; First, drawings longer than the max_seq_length are filtered out. Then, outlier delta values are clipped, and finally, the data is normalized using the pre-calculated standard deviation.\n",
        "2. <b>__ getitem __</b>; When a sample is requested, it is randomly selected and augmented based on the provided parameters. A single stroke is selected from the drawing. The selected stroke is used to fed the diffusion model and the previous strokes is used for Storhe History model. In this case, special SOS (Start of Sequence) token is only prepended to the stroke history. Also both stroke_history and stroke padded* with zeros to the max_seq_length and max_single_stroke_length.\n",
        "\n",
        "I set 96 as a maximum sequence length. This value is copied from [SketchKnitter](https://openreview.net/pdf?id=4eJ43EN2g6l) paper which is designed for the same purpose. Maximum single stroke is calculated after preprocessing step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create DataLoader for training and testing sets\n",
        "from torch.utils.data import DataLoader\n",
        "from src.modules.dataset import StrokeHistoryQuickDrawDataset\n",
        "\n",
        "\n",
        "history_train_dataset = StrokeHistoryQuickDrawDataset(\n",
        "    drawing_5d=train_set_5d,\n",
        "    max_seq_length=96, # Copies from the reference implementation\n",
        "    std=std,\n",
        "    random_scale_factor=0.15, # Copies from the reference implementation\n",
        "    augment_stroke_prob=0.10, # Copies from the reference implementation,\n",
        "    limit=1000, # Copies from the reference implementation\n",
        ")\n",
        "\n",
        "history_test_dataset = StrokeHistoryQuickDrawDataset(\n",
        "    drawing_5d=test_set_5d,\n",
        "    max_seq_length=96, # Copies from the reference implementation\n",
        "    std=std,\n",
        "    random_scale_factor=0.15, # Copies from the reference implementation\n",
        "    augment_stroke_prob=0.10, # Copies from the reference implementation,\n",
        "    limit=1000, # Copies from the reference implementation\n",
        ")\n",
        "\n",
        "history_train_dataloader = DataLoader(\n",
        "    dataset=history_train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "history_test_dataloader = DataLoader(\n",
        "    dataset=history_test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "for test_batch in history_train_dataloader:\n",
        "    print(f\"Batch Shape: {test_batch['stroke'].shape}\")\n",
        "    print(f\"Batch Sample: {test_batch['stroke'][0, :5]}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Stroke History Conditioned Diffusion Transformer\n",
        "For the final experiment, I moved from a holistic generation model to a more advanced hierarchical and autoregressive architecture. This approach uses an additional network to encode the drawing history, providing a rich context to the Diffusion Transformer via cross-attention.\n",
        "\n",
        "1. The stroke history is first fed into an LSTM-based StrokeHistoryEncoder. This network uses a bidirectional LSTM, which increases its power to extract complex relationships between the strokes by processing the sequence in both forward and reverse directions. A padding mask is provided to this encoder, which is used within its attention layers to ensure that the model does not form relationships between real stroke data and the meaningless padded areas.\n",
        "2. The features extracted by the encoder, which represent a contextual summary of the drawing's past, are then fed as a condition to the DiffusionTransformer. The Transformer blocks in this model are equipped with cross-attention layers, a powerful and popular method for conditioning diffusion models on external information like text or, in this case, a drawing history.\n",
        "3. The DiffusionTransformer's main task is to denoise the next target stroke. It takes the noisy stroke as input and, guided by the context from the history encoder, predicts the noise. Similar to the first unconditional approach, it uses two separate output heads to predict the noise for the delta values and the logits for the pen states.\n",
        "\n",
        "The primary drawback of this autoregressive architecture is its slow sampling speed. Because the model generates sketches sequentially‚Äîproducing only one stroke at a time based on all previous strokes‚Äîthe overall inference time scales linearly with the number of strokes in the final drawing. This iterative process is computationally expensive and makes generation significantly more time-consuming compared to holistic approaches that produce the entire sketch in a single pass.\n",
        "\n",
        "![Stroke History Dit Architecture](assets/stroke_history_dit.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Diffusion Model for First Approach\n",
        "from src.models.history_encoder import StrokeHistoryEncoder\n",
        "from src.models.dit import DiffusersBlockCrossAttentionTransformer\n",
        "\n",
        "history_encoder = StrokeHistoryEncoder()\n",
        "diffusion_model_wcrossattn = DiffusersBlockCrossAttentionTransformer(\n",
        "    input_feats=2,\n",
        "    output_feats=2,\n",
        "    pen_state_feats=3,\n",
        "    pen_condition=True # Set to True for pen state conditioning\n",
        ")\n",
        "\n",
        "history_encoder.to(DEVICE)\n",
        "diffusion_model_wcrossattn.to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training\n",
        "\n",
        "In this section I provide my training strategy. I prefer to share my functions in a cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CONSTANTS\n",
        "import torch\n",
        "\n",
        "NUM_STEPS = 50000\n",
        "NUM_TIMESTEPS = 100\n",
        "LEARNING_RATE = 1e-4\n",
        "MIN_LEARNING_RATE = 1e-6\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To maintain consistency across different experiments, I encapsulated the core training logic within a single single_step function. This function is designed to be flexible, with parameters like <i>pen_condition</i> that can be toggled for different architectural approaches. Below is a description of this function and its key components.\n",
        "\n",
        "<b>rescale_timesteps</b> <br>\n",
        "This function is an implementation of a technique originally used in the [SketchKnitter](https://openreview.net/pdf?id=4eJ43EN2g6l) paper. It rescales the diffusion timesteps, which can improve sampling stability and efficiency. The paper proposes that using a smaller number of timesteps for sampling (inference) is more efficient. Following their findings, I set the <i>NUM_TIMESTEPS</i> hyperparameter to 100 for my experiments.\n",
        "\n",
        "<b>single_step</b> <br>\n",
        "This function handles a single forward pass of the training process, including the loss calculation. The total loss is composed of two distinct parts, creating a multi-task objective:\n",
        "\n",
        "- MSE Loss: Used for the primary denoising task on the continuous delta values (Œîx, Œîy).\n",
        "- Cross-Entropy Loss: Used for the pen state classification task (p1, p2, p3), when this feature is active.\n",
        "\n",
        "These two losses are combined using a lambda coefficient. For this project, I set lambda to 0.01, which weighs the contribution of the pen state loss. This weighting strategy is also adapted from the [SketchKnitter](https://openreview.net/pdf?id=4eJ43EN2g6l) paper.\n",
        "\n",
        "<b>train_loop</b> <br>\n",
        "This function orchestrates the entire training process over a specified number of epochs. Within each epoch, it executes two main phases: a training step, where the model's weights are updated using the training data, and a validation step, where the model's performance is evaluated on unseen data to monitor for overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Training Functions\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def rescale_timesteps(t: torch.Tensor,\n",
        "                      num_timestep: int = 100):\n",
        "    \n",
        "    return t.float() * (1000.0 / num_timestep)\n",
        "\n",
        "def single_step(batch: dict,\n",
        "                diffusion_model: torch.nn.Module,\n",
        "                scheduler: torch.nn.Module,\n",
        "                history_encoder: torch.nn.Module = None,\n",
        "                pen_condition: bool = True):\n",
        "    \n",
        "    stroke = batch[\"stroke\"]\n",
        "    stroke = stroke.to(DEVICE).float()\n",
        "\n",
        "    # Get real pen state\n",
        "    real_pen_state = stroke[:, :, 2:]\n",
        "    stroke = stroke[:, :, :2]\n",
        "\n",
        "    if history_encoder is not None:\n",
        "        stroke_history = batch[\"stroke_history\"].to(DEVICE).float()\n",
        "        stroke_history_mask = batch[\"stroke_history_mask\"].to(DEVICE).float()\n",
        "\n",
        "        stroke_history_features = history_encoder(\n",
        "            stroke_history,\n",
        "            stroke_history_mask\n",
        "        )\n",
        "\n",
        "    # Add noise to the stroke\n",
        "    noise = torch.randn_like(stroke)\n",
        "    timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (stroke.shape[0],), device=DEVICE).long()\n",
        "    noisy_stroke = scheduler.add_noise(stroke, noise, timesteps)\n",
        "\n",
        "    # Prepare model input\n",
        "    if pen_condition:\n",
        "        model_input = torch.cat([noisy_stroke, real_pen_state], dim=-1)\n",
        "    else:\n",
        "        model_input = noisy_stroke\n",
        "    \n",
        "    # Forward pass through the diffusion model\n",
        "    predicted_noise, pen_state_out = diffusion_model(\n",
        "            model_input,\n",
        "            rescale_timesteps(timesteps),\n",
        "            context=stroke_history_features if history_encoder is not None else None\n",
        "        )\n",
        "    \n",
        "    # Calculate the loss\n",
        "    loss_delta = F.mse_loss(\n",
        "        predicted_noise,\n",
        "        noise\n",
        "    )\n",
        "\n",
        "    if not pen_condition:\n",
        "        B, C = real_pen_state.shape[:2]\n",
        "        pen_state_out = pen_state_out.reshape(B * C, 3).type(torch.FloatTensor).to(DEVICE)\n",
        "        real_pen_state = real_pen_state.reshape(B * C, 3).type(torch.FloatTensor).to(DEVICE)\n",
        "\n",
        "        loss_pen = F.cross_entropy(\n",
        "            pen_state_out,\n",
        "            real_pen_state,\n",
        "            )\n",
        "    else:\n",
        "        loss_pen = 0\n",
        "\n",
        "    loss = loss_delta + 0.01 * loss_pen\n",
        "    return loss, loss_delta, loss_pen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "def train_loop(\n",
        "      diffusion_model: torch.nn.Module, \n",
        "      optimizer: torch.optim.Optimizer, \n",
        "      scheduler: torch.optim.lr_scheduler.LambdaLR, \n",
        "      scheduler_lr: torch.optim.lr_scheduler.LambdaLR, \n",
        "      train_dataloader: torch.utils.data.DataLoader, \n",
        "      test_dataloader: torch.utils.data.DataLoader, \n",
        "      history_encoder: torch.nn.Module = None,\n",
        "      pen_condition: bool = True):\n",
        "  \n",
        "  # Initialize the gradient scaler for mixed precision training\n",
        "  scaler = torch.amp.GradScaler(\"cuda\")\n",
        "\n",
        "  total_step = 0\n",
        "  total_train_loss = 0.0\n",
        "  total_test_loss = 0.0\n",
        "  train_loop = tqdm(range(NUM_STEPS), desc=f\"Training\")\n",
        "  while total_step < 50001:\n",
        "      \n",
        "    # Training steps\n",
        "    diffusion_model.train()\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        with torch.amp.autocast(\"cuda\"):\n",
        "            loss, loss_delta, loss_pen = single_step(\n",
        "                batch, diffusion_model, scheduler, pen_condition=pen_condition, history_encoder=history_encoder\n",
        "            )\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        train_loop.set_postfix(\n",
        "            train_loss_delta=loss_delta.item(),\n",
        "            train_loss=loss.item(),\n",
        "            train_loss_pen=loss_pen.item() if loss_pen != 0 else 0\n",
        "            )\n",
        "\n",
        "        total_step += 1\n",
        "        scheduler_lr.step()\n",
        "\n",
        "    # Test steps\n",
        "    diffusion_model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            test_loss, test_loss_delta, test_loss_pen = single_step(\n",
        "                batch, diffusion_model, scheduler, pen_condition=pen_condition, history_encoder=history_encoder\n",
        "                )\n",
        "\n",
        "\n",
        "        total_test_loss += test_loss.item()\n",
        "        train_loop.set_postfix(\n",
        "            train_loss_delta=test_loss_delta.item(),\n",
        "            train_loss=test_loss.item(),\n",
        "            train_loss_pen=test_loss_pen.item() if test_loss_pen != 0 else 0\n",
        "            )\n",
        "\n",
        "    if total_step % 1000 == 0:\n",
        "        avg_train_loss = total_train_loss / total_step\n",
        "        avg_test_loss = total_test_loss / total_step\n",
        "        print(f\"Train Loss = {avg_train_loss:.4f} | Validation Loss = {avg_test_loss:.4f}\")\n",
        "\n",
        "    train_loop.update(1)\n",
        "train_loop.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<b>set_training_parameters</b> <br>\n",
        "This helper function centralizes the initialization of core training components. It sets up the DDIMScheduler for the diffusion process and the CosineAnnealingLR to manage the learning rate decay over the training run. Crucially, it configures the AdamW optimizer to handle both the unconditional and conditional training scenarios. \n",
        "\n",
        "If a history_encoder is provided, the function intelligently combines the parameters from both the diffusion model and the encoder, ensuring that the entire architecture is trained end-to-end. If no encoder is present, it configures the optimizer for the diffusion model alone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up schedulers and optimizers\n",
        "from itertools import chain\n",
        "from diffusers import DDIMScheduler\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "\n",
        "def set_training_parameters(diffusion_model, train_dataloader, history_encoder=None):\n",
        "    scheduler = DDIMScheduler(\n",
        "        num_train_timesteps=NUM_TIMESTEPS,\n",
        "        beta_schedule='linear',\n",
        "        prediction_type='epsilon'\n",
        "    )\n",
        "\n",
        "    if history_encoder is not None:\n",
        "        optimizer = torch.optim.AdamW(chain(\n",
        "            diffusion_model.parameters(),\n",
        "            history_encoder.parameters()\n",
        "        ), lr=LEARNING_RATE)\n",
        "    else:\n",
        "        optimizer = torch.optim.AdamW(diffusion_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    scheduler_lr = CosineAnnealingLR(optimizer, T_max=NUM_TIMESTEPS, eta_min=MIN_LEARNING_RATE)\n",
        "\n",
        "    return scheduler, optimizer, scheduler_lr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training and Sampling for Unconditional or Condition Diffusion Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scheduler, optimizer, scheduler_lr = set_training_parameters(\n",
        "    diffusion_model=diffusion_model,\n",
        "    train_dataloader=train_dataloader\n",
        ")\n",
        "\n",
        "# Unconditionally train the diffusion model\n",
        "train_loop(\n",
        "    diffusion_model=diffusion_model,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    scheduler_lr=scheduler_lr,\n",
        "    train_dataloader=train_dataloader,\n",
        "    test_dataloader=test_dataloader,\n",
        "    pen_condition=True \n",
        ")\n",
        "\n",
        "# Contion with pen state train the diffusion model\n",
        "train_loop(\n",
        "    diffusion_model=diffusion_model,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    scheduler_lr=scheduler_lr,\n",
        "    train_dataloader=train_dataloader,\n",
        "    test_dataloader=test_dataloader,\n",
        "    pen_condition=True \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Sampling\n",
        "\n",
        "This code block executes the full inference pipeline, starting from pure noise and resulting in a final, animated visualization of a generated sketch. The process begins with the dit_sampling function, which initializes a tensor with random Gaussian noise. This tensor is then iteratively refined over 100 denoising steps, where the trained diffusion model predicts the noise at each step, and the scheduler uses that prediction to produce a slightly cleaner version of the sketch data. Once the sampling loop is complete, the raw numerical output is post-processed to convert the pen state logits into clean one-hot vectors. Finally, this complete 5D vector sequence is denormalized and converted back into a raw stroke format, which is then rendered as an animated GIF to display the model's sequential drawing process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "def dit_sampling(diffusion_model: torch.nn.Module,\n",
        "             scheduler: torch.nn.Module,\n",
        "             timestep: int = 100,\n",
        "             max_seq_length: int = 96 + 1,\n",
        "             pen_condition_value: torch.Tensor = None):\n",
        "    diffusion_model.eval()\n",
        "\n",
        "    sample_shape = (1, max_seq_length, 2)\n",
        "    sample = torch.randn(sample_shape, device=DEVICE)\n",
        "    sample[:, 0, :] = torch.Tensor([0,0])\n",
        "\n",
        "    if pen_condition_value is None:\n",
        "        sample = sample\n",
        "\n",
        "    else:\n",
        "        sample = torch.cat([sample, pen_condition_value.unsqueeze(-1)], dim=-1)\n",
        "\n",
        "    scheduler.set_timesteps(timestep)\n",
        "    for t in tqdm(scheduler.timesteps):\n",
        "        with torch.no_grad():\n",
        "            predicted_noise, pen_values = diffusion_model(sample, t.reshape(1,).to(DEVICE))\n",
        "\n",
        "        sample = scheduler.step(predicted_noise, t, sample).prev_sample\n",
        "\n",
        "    return sample, pen_values\n",
        "\n",
        "def postprocess_drawing_torch(pen_state_logits):\n",
        "    probabilities = torch.softmax(pen_state_logits, dim=-1)\n",
        "    winner_indices = torch.argmax(probabilities, dim=-1)\n",
        "    clean_pen_states = F.one_hot(winner_indices, num_classes=3).float()\n",
        "    return clean_pen_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "from src.utils.dataset_utils import denormalize, convert_5d_to_raw_format, generate_gif\n",
        "\n",
        "predicted_delta_values, predicted_pen_values = dit_sampling(\n",
        "    diffusion_model=diffusion_model,\n",
        "    scheduler=scheduler,\n",
        "    timestep=100,\n",
        "    max_seq_length=96 + 1,\n",
        "    pen_condition_value=None\n",
        ")\n",
        "\n",
        "if predicted_pen_values is not None:\n",
        "  predicted_pen_values = postprocess_drawing_torch(predicted_pen_values)\n",
        "  final_output = torch.cat([predicted_delta_values, predicted_pen_values], dim=-1)\n",
        "\n",
        "\n",
        "denormalized = denormalize(final_output, std)\n",
        "raw_drawing = convert_5d_to_raw_format(denormalized.detach().cpu().numpy()[0])\n",
        "generate_gif(raw_drawing, \"./test\", fps=25)\n",
        "Image(filename=\"./test/drawing.gif\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training and Sampling for Stroke History based Diffusion Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scheduler, optimizer, scheduler_lr = set_training_parameters(\n",
        "    diffusion_model=diffusion_model_wcrossattn,\n",
        "    train_dataloader=history_train_dataloader,\n",
        "    history_encoder=history_encoder\n",
        ")\n",
        "\n",
        "# Stroke History based diffusion model training\n",
        "train_loop(\n",
        "    diffusion_model=diffusion_model_wcrossattn,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    scheduler_lr=scheduler_lr,\n",
        "    train_dataloader=history_train_dataloader,\n",
        "    test_dataloader=history_test_dataloader,\n",
        "    history_encoder=history_encoder,\n",
        "    pen_condition=False \n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Sampling\n",
        "\n",
        "This code demonstrates the complete inference pipeline for Stroke History based Diffusion Transformer Model architecture. Rather than generating the entire sketch at once, this method constructs the drawing sequentially, stroke-by-stroke, to mimic a human-like process. The main for loop iterates for a set number of steps (STEP_COUNT), calling the stroke_history_dit_sampling function in each iteration to generate a single new stroke. The prev_generated_stroke_history variable manages the state of the drawing; it starts empty, is updated with each newly generated stroke, and is then passed as the condition for the next generation step. This process relies on the collaboration of two models: the stroke_history_encoder interprets the current history into a rich context vector, while the cross-attention diffusion_model uses this context to denoise a random signal into the next coherent stroke. Finally, after all strokes are generated, they are concatenated and post-processed to render an animated GIF that visualizes the entire sequential drawing process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "def stroke_history_dit_sampling(\n",
        "    diffusion_model_wcrossattn: torch.nn.Module,\n",
        "    stroke_history_encoder: torch.nn.Module,\n",
        "    scheduler: torch.nn.Module,\n",
        "    timestep: int = 100,\n",
        "    max_seq_length: int = 96 + 1,\n",
        "    max_single_stroke_length: int = 44,\n",
        "    prev_generated_stroke_history = None\n",
        "    ):\n",
        "  \n",
        "    diffusion_model.eval()\n",
        "    stroke_history_encoder.eval()\n",
        "\n",
        "    stroke_history_shape = (1, max_seq_length, 5)\n",
        "    stroke_history = torch.zeros(stroke_history_shape, device=DEVICE)\n",
        "    stroke_history[:, 0, :] = torch.Tensor([0, 0, 1, 0, 0])\n",
        "    stroke_history_mask = torch.zeros(stroke_history_shape[:2], device=DEVICE)\n",
        "    stroke_history_mask[:, 0] = 1\n",
        "\n",
        "    if prev_generated_stroke_history is not None:\n",
        "        stroke_history[:, 1:prev_generated_stroke_history.shape[1]+1, :] = prev_generated_stroke_history\n",
        "        stroke_history_mask[:, 1:prev_generated_stroke_history.shape[1]+1] = 1\n",
        "\n",
        "    stroke_history_features = stroke_history_encoder(\n",
        "        stroke_history,\n",
        "        stroke_history_mask\n",
        "    )\n",
        "\n",
        "\n",
        "    sample_stroke_shape = (1, max_single_stroke_length, 2)\n",
        "    sample = torch.randn(sample_stroke_shape, device=DEVICE)\n",
        "\n",
        "\n",
        "    scheduler.set_timesteps(timestep)\n",
        "    for t in tqdm(scheduler.timesteps):\n",
        "        with torch.no_grad():\n",
        "            predicted_noise, pen_values = diffusion_model_wcrossattn(\n",
        "                sample,\n",
        "                t.reshape(1,).to(DEVICE),\n",
        "                context=stroke_history_features\n",
        "              )\n",
        "\n",
        "        sample = scheduler.step(predicted_noise, t, sample).prev_sample\n",
        "\n",
        "    return predicted_noise, pen_values\n",
        "\n",
        "def postprocess_drawing_torch(pen_state_logits):\n",
        "    probabilities = torch.softmax(pen_state_logits, dim=-1)\n",
        "    winner_indices = torch.argmax(probabilities, dim=-1)\n",
        "    clean_pen_states = F.one_hot(winner_indices, num_classes=3).float()\n",
        "    return clean_pen_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "from src.utils.dataset_utils import denormalize, convert_5d_to_raw_format, generate_gif\n",
        "\n",
        "\n",
        "STEP_COUNT = 2\n",
        "prev_generated_stroke_history = None\n",
        "for _ in range(STEP_COUNT):\n",
        "    predicted_delta_values, predicted_pen_values = stroke_history_dit_sampling(\n",
        "        diffusion_model_wcrossattn=diffusion_model_wcrossattn,\n",
        "        stroke_history_encoder=history_encoder,\n",
        "        scheduler=scheduler,\n",
        "        timestep=100,\n",
        "        max_seq_length=96 + 1,\n",
        "        max_single_stroke_length= 44,\n",
        "        prev_generated_stroke_history = prev_generated_stroke_history\n",
        "    )\n",
        "\n",
        "    predicted_pen_values = postprocess_drawing_torch(predicted_pen_values)\n",
        "    final_output = torch.cat([predicted_delta_values, predicted_pen_values], dim=-1)\n",
        "\n",
        "    if prev_generated_stroke_history is None:\n",
        "        prev_generated_stroke_history = final_output\n",
        "    else:\n",
        "        prev_generated_stroke_history = torch.cat([prev_generated_stroke_history, final_output], dim=1)\n",
        "\n",
        "\n",
        "denormalized = denormalize(prev_generated_stroke_history, std)\n",
        "raw_drawing = convert_5d_to_raw_format(denormalized.detach().cpu().numpy()[0])\n",
        "generate_gif(raw_drawing, \"./test\", fps=25)\n",
        "Image(filename=\"./test/drawing.gif\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For quantitative result forthe quality and diversity of the generated sketches, I implemented the function to calculate two standard metrics: <b>Fr√©chet Inception Distance (FID)</b> and <b>Kernel Inception Distance (KID)</b>. \n",
        "\n",
        "The <b>calculate_fid_kid</b> function is designed to measure the distributional similarity between a set of real sketches from the test data and a set of sketches produced by the trained model. The workflow involves first rendering both sets of sketches as images and saving them into separate directories. This function then uses the torch-fidelity library to compare these two image sets. Lower FID and KID scores indicate that the generated sketches are statistically more similar to real sketches.\n",
        "\n",
        "<b><u>However, this function was not executed for the final evaluation. Since the model did not converge to a state where it produced recognizable sketches, running a comparative analysis like FID/KID would not produce meaningful results.</u></b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch_fidelity import calculate_metrics\n",
        "\n",
        "def calculate_fid_kid(real_images_dir: str, generated_images_dir: str):    \n",
        "    metrics_dict = calculate_metrics(\n",
        "        input1=real_images_dir,\n",
        "        input2=generated_images_dir,\n",
        "        cuda=(DEVICE == 'cuda'),\n",
        "        isc=False,\n",
        "        fid=True,\n",
        "        kid=True,\n",
        "        verbose=False\n",
        "    )\n",
        "    \n",
        "    fid_score = metrics_dict['frechet_inception_distance']\n",
        "    kid_mean, kid_std = metrics_dict['kernel_inception_distance_mean'], metrics_dict['kernel_inception_distance_std']\n",
        "\n",
        "    return fid_score, (kid_mean, kid_std)\n",
        "\n",
        "# Calculate FID and KID scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VEme3no2Xpc"
      },
      "source": [
        "# References\n",
        "\n",
        "- [SketchRNN](https://arxiv.org/abs/1704.03477)\n",
        "- [SketchKnitter](https://openreview.net/pdf?id=4eJ43EN2g6l)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
